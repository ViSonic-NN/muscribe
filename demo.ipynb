{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "\n",
    "from muscribe import midi2score\n",
    "from muscribe.audio2midi import PianoTranscription\n",
    "\n",
    "\n",
    "def get_midi(audio_path, output_midi_path):\n",
    "    import time\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    transcriptor = PianoTranscription(device=device)\n",
    "    transcribe_time = time.time()\n",
    "    transcriptor.transcribe(audio_path, output_midi_path)\n",
    "    print(\"Transcribe time: {:.3f} s\".format(time.time() - transcribe_time))\n",
    "    if output_midi_path:\n",
    "        return midi2score.read_midi_notes(output_midi_path)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_beats(audio_path: str, offset: float = 0.0, duration: float | None = None):\n",
    "    import numpy as np\n",
    "    from BeatNet.BeatNet import BDA, BeatNet\n",
    "    from madmom.features.downbeats import DBNDownBeatTrackingProcessor\n",
    "\n",
    "    beatnet = BeatNet(model=1)\n",
    "    audio, _ = librosa.load(\n",
    "        audio_path, sr=beatnet.sample_rate, offset=offset, duration=duration\n",
    "    )\n",
    "    feats = beatnet.proc.process_audio(audio).T\n",
    "    feats = torch.from_numpy(feats).unsqueeze(0).to(beatnet.device)\n",
    "    bn_model: BDA = beatnet.model  # type: ignore\n",
    "    preds = bn_model.final_pred(bn_model(feats)[0])\n",
    "    preds = preds.cpu().detach().numpy()\n",
    "    beat_activ = np.transpose(preds[:2, :])\n",
    "\n",
    "    db_tracker = DBNDownBeatTrackingProcessor(beats_per_bar=[4], fps=50)\n",
    "    return db_tracker(beat_activ)  # Using DBN offline inference to infer beat/downbeats\n",
    "\n",
    "\n",
    "def get_keysig(midi_notes):\n",
    "    key_sig_pro = midi2score.RNNKeySignatureProcessor()\n",
    "    return key_sig_pro.process(midi_notes)\n",
    "\n",
    "\n",
    "def get_hand_parts(midi_notes):\n",
    "    hand_parts_pro = midi2score.RNNHandPartProcessor()\n",
    "    return hand_parts_pro.process(midi_notes)\n",
    "\n",
    "\n",
    "midi = get_midi(\"example/sonatine.mp3\", \"example/sonatine.midi\")\n",
    "midi = midi2score.read_midi_notes(\"example/sonatine.midi\", offset=60, duration=120)\n",
    "beats = get_beats(\"example/sonatine.mp3\", offset=60, duration=120)\n",
    "key_change = get_keysig(midi)\n",
    "hand_parts = get_hand_parts(midi)\n",
    "builder = midi2score.MusicXMLBuilder(beats)\n",
    "builder.add_notes(midi.numpy(), hand_parts.numpy())\n",
    "builder.add_key_changes(key_change)\n",
    "builder.infer_bpm_changes(diff_size=2, log_bin_size=0.03)\n",
    "builder.render(\"example/sonatine.192+120.xml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "513c34b582369c042e4c09b2082a0eee972fc3557bcb2c161c81e814cba2f5b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
