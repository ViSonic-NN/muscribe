{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire training code:\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from midi_score import BeatPredictorPL\n",
    "\n",
    "model = BeatPredictorPL(\"midi_score/dataset\")\n",
    "pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=100).fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([154, 200, 128]), torch.Size([154, 200]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting dataset: peek at the first batch\n",
    "from midi_score import BeatPredictorPL\n",
    "\n",
    "model = BeatPredictorPL(\"midi_score/dataset\")\n",
    "train_loader = model.train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "notes, (beats, ) = batch\n",
    "notes.shape, beats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/local/yifanz16/packages/mamba/envs/muscribe/lib/python3.11/site-packages/torchinfo/torchinfo.py:373: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/srv/local/yifanz16/packages/mamba/envs/muscribe/lib/python3.11/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "BeatPredictorPL                                    --                        --\n",
       "├─BeatTransformer: 1                               --                        --\n",
       "│    └─TransformerEncoder: 2                       --                        --\n",
       "│    │    └─ModuleList: 3-1                        --                        529,920\n",
       "├─BeatTransformer: 1-1                             [170, 200, 2]             --\n",
       "│    └─Linear: 2-1                                 [200, 170, 128]           16,512\n",
       "│    └─PositionalEncoding: 2-2                     [170, 200, 128]           --\n",
       "│    │    └─Dropout: 3-2                           [170, 200, 128]           --\n",
       "│    └─TransformerEncoder: 2-3                     [170, 200, 128]           --\n",
       "│    └─Linear: 2-4                                 [170, 200, 2]             258\n",
       "====================================================================================================\n",
       "Total params: 16,770\n",
       "Trainable params: 16,770\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 3.35\n",
       "====================================================================================================\n",
       "Input size (MB): 17.41\n",
       "Forward/backward pass size (MB): 35.36\n",
       "Params size (MB): 0.07\n",
       "Estimated Total Size (MB): 52.84\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "\n",
    "# Check model size\n",
    "torchinfo.summary(model, input_data=batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi as pm\n",
    "\n",
    "midi = pm.PrettyMIDI(\"midi_score/dataset/asap/Bach/Italian_concerto/KyykhynenT03.mid\")\n",
    "piano = midi.instruments[0]\n",
    "for note in piano.notes:\n",
    "    print(note.start, note.end, note.pitch, note.velocity, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def midi_to_encoded_with_annots(midi_data, annots, interval=0.05):\n",
    "    def get_previous_beat(onset, beats):\n",
    "        \"\"\"Helper function to get the previous beat for a given onset time\"\"\"\n",
    "        return max(beat for beat in beats if beat <= onset)\n",
    "\n",
    "    # Helper function to get beat duration around the given onset time\n",
    "    def get_beat_duration(onset, beats):\n",
    "        # Find the nearest beats before and after the onset\n",
    "        previous_beat = max(beat for beat in beats if beat <= onset)\n",
    "        next_beat = min(beat for beat in beats if beat > onset)\n",
    "\n",
    "        return next_beat - previous_beat\n",
    "\n",
    "    # Find the total duration required\n",
    "    total_duration = max(\n",
    "        note[1] + note[2] for note in midi_data\n",
    "    )  # considering the note's offset\n",
    "    length = int(total_duration / interval) + 1\n",
    "\n",
    "    # Create an encoding matrix filled with zeros\n",
    "    encoding = torch.zeros(128 + 1 + 1 + 16 + 9 + 12 + 1 + 1, length)\n",
    "\n",
    "    # Populate the encoding for the notes from midi_data\n",
    "    for idx, note in enumerate(midi_data):\n",
    "        pitch, onset, _, _ = note\n",
    "        onset -= annots[4][idx]\n",
    "        beat_duration = get_beat_duration(\n",
    "            onset, annots[0]\n",
    "        )  # get beat duration surrounding this note\n",
    "        adjusted_duration = (\n",
    "            beat_duration * annots[5][idx]\n",
    "        )  # adjusting the duration with its note value\n",
    "        start_idx = int(torch.round(onset / interval).item())\n",
    "        end_idx = int(torch.round((onset + adjusted_duration) / interval).long().item())\n",
    "        encoding[pitch, start_idx:end_idx] = 1\n",
    "\n",
    "    # Populate the encoding for the annotations\n",
    "    beats, downbeats, time_signatures, key_signatures, onsets_musical, _, hands = annots\n",
    "\n",
    "    for beat in beats:\n",
    "        idx = int(beat / interval)\n",
    "        encoding[128, idx] = 1\n",
    "\n",
    "    for downbeat in downbeats:\n",
    "        idx = int(downbeat / interval)\n",
    "        encoding[129, idx] = 1\n",
    "\n",
    "    for ts in time_signatures:\n",
    "        time, numerator, denominator = ts\n",
    "        idx = int(time / interval)\n",
    "        encoding[130 + numerator - 1, idx] = 1  # Numerator encoding\n",
    "\n",
    "        # Denominator encoding\n",
    "        denominator_indices = {\n",
    "            1: 0,\n",
    "            2: 1,\n",
    "            4: 2,\n",
    "            8: 3,\n",
    "            16: 4,\n",
    "            32: 5,\n",
    "            64: 6,\n",
    "            128: 7,\n",
    "            256: 8,\n",
    "        }\n",
    "        encoding[146 + denominator_indices[denominator], idx] = 1\n",
    "\n",
    "    for ks in key_signatures:\n",
    "        time, key_number = ks\n",
    "        idx = int(time / interval)\n",
    "        encoding[155 + key_number, idx] = 1\n",
    "\n",
    "    for idx, onset in enumerate(annots[4]):\n",
    "        previous_beat = get_previous_beat(midi_data[idx][1], annots[0])\n",
    "        relative_onset = previous_beat + onset\n",
    "        idx = int(relative_onset / interval)\n",
    "        encoding[167, idx] = 1\n",
    "\n",
    "    for hand in hands:\n",
    "        time, hand_type = hand\n",
    "        idx = int(time / interval)\n",
    "        encoding[168, idx] = hand_type\n",
    "\n",
    "    return encoding\n",
    "\n",
    "\n",
    "# Example usage\n",
    "midi_data = [\n",
    "    (60, 0.5, 0.2, 50),  # C4 note with onset at 0.5s, duration 0.2s, and velocity 50\n",
    "    # ... (other notes)\n",
    "]\n",
    "\n",
    "annots = [\n",
    "    [0.3, 0.5, 0.8, 1.5],  # Beats\n",
    "    [0.5],  # Downbeats\n",
    "    [(0, 4, 4), (5, 3, 4)],  # Time signatures\n",
    "    [(0, 5)],  # Key signatures\n",
    "    [0.5],  # Onsets musical\n",
    "    [2],  # Note values (this note will have twice its original length in beats)\n",
    "    [(0.5, 1)],  # Hands\n",
    "]\n",
    "\n",
    "encoded_tensor = midi_to_encoded_with_annots(notes, lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_tensor_structure(tensor_list):\n",
    "    for i, tensor in enumerate(tensor_list):\n",
    "        print(f\"Tensor {i + 1}:\")\n",
    "        print(f\"  Shape: {tensor.shape}\")\n",
    "        print(f\"  Data type: {tensor.dtype}\")\n",
    "        print(f\"  first shape: {tensor[0]}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "inspect_tensor_structure(lables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visonic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
