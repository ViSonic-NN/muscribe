{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire training code:\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from midi_score import BeatPredictorPL\n",
    "\n",
    "#trading-off precision for speed with tensor cores, enable if you have tensor cores\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "model = BeatPredictorPL(\"midi_score/dataset\", 100)\n",
    "pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=100).fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting dataset: peek at the first batch\n",
    "from midi_score import BeatPredictorPL\n",
    "import madmom.features\n",
    "\n",
    "model = BeatPredictorPL(\"midi_score/dataset\", 100)\n",
    "val_loader = model.cpu().val_dataloader()\n",
    "batch= next(iter(val_loader))\n",
    "batch[0].shape\n",
    "#notes, (beats, ) = \n",
    "#notes.shape, beats.shape\n",
    "proc = madmom.features.DBNBeatTrackingProcessor(fps = 50)\n",
    "#model.forward(batch[0])[1,:,0]\n",
    "print(proc(model.forward(batch[0])[1,:,0].detach()))\n",
    "\n",
    "print(batch[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "from torch.nn import TransformerEncoderLayer\n",
    "\n",
    "# Check model size\n",
    "lm = torchinfo.torchinfo.LAYER_MODULES\n",
    "torchinfo.torchinfo.LAYER_MODULES = (*lm, TransformerEncoderLayer)\n",
    "summary = torchinfo.summary(model.model, input_data=batch[0])\n",
    "# See TransformerEncoderLayer as a leaf layer even though it has submodules\n",
    "for info in summary.summary_list:\n",
    "    info.is_leaf_layer = info.is_leaf_layer or type(info.module) in lm\n",
    "torchinfo.ModelStatistics(summary.summary_list, summary.input_size, summary.total_input, summary.formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi as pm\n",
    "\n",
    "midi = pm.PrettyMIDI(\"midi_score/dataset/asap/Bach/Italian_concerto/KyykhynenT03.mid\")\n",
    "piano = midi.instruments[0]\n",
    "for note in piano.notes:\n",
    "    print(note.start, note.end, note.pitch, note.velocity, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def encode_notes(midi_data, interval, cutoff):\n",
    "    # Find the total duration required\n",
    "    total_duration = max(\n",
    "        note[1] + note[2] for note in midi_data[midi_data[:, 1] < cutoff]\n",
    "    )  # considering the note's offset\n",
    "    length = int(total_duration / interval)\n",
    "    # Create an encoding matrix filled with zeros\n",
    "    encoding = torch.zeros(length, 128)\n",
    "    # Populate the encoding for the notes from midi_data\n",
    "    for idx, note in enumerate(midi_data[midi_data[:, 1] < cutoff]):\n",
    "        pitch, onset, duration, _ = note\n",
    "        start_idx = int((onset.item() // interval))\n",
    "        end_idx = int(((onset.item() + duration.item()) // interval))\n",
    "        encoding[ start_idx:end_idx, int(pitch.item())] = 1\n",
    "\n",
    "    return encoding\n",
    "\n",
    "def midi_to_encoded_with_annots(midi_data, annots, interval=0.05):\n",
    "    def get_previous_beat(onset, beats):\n",
    "        \"\"\"Helper function to get the previous beat for a given onset time\"\"\"\n",
    "        return max(beat for beat in beats if beat <= onset)\n",
    "\n",
    "    # Helper function to get beat duration around the given onset time\n",
    "    def get_beat_duration(onset, beats):\n",
    "        # Find the nearest beats before and after the onset\n",
    "        previous_beat = max(beat for beat in beats if beat <= onset)\n",
    "        next_beat = min(beat for beat in beats if beat > onset)\n",
    "\n",
    "        return next_beat - previous_beat\n",
    "\n",
    "    # Find the total duration required\n",
    "    total_duration = max(\n",
    "        note[1] + note[2] for note in midi_data\n",
    "    )  # considering the note's offset\n",
    "    length = int(total_duration / interval) + 1\n",
    "\n",
    "    # Create an encoding matrix filled with zeros\n",
    "    encoding = torch.zeros(128 + 1 + 1 + 16 + 9 + 12 + 1 + 1, length)\n",
    "\n",
    "    # Populate the encoding for the notes from midi_data\n",
    "    for idx, note in enumerate(midi_data):\n",
    "        pitch, onset, _, _ = note\n",
    "        onset -= annots[4][idx]\n",
    "        beat_duration = get_beat_duration(\n",
    "            onset, annots[0]\n",
    "        )  # get beat duration surrounding this note\n",
    "        adjusted_duration = (\n",
    "            beat_duration * annots[5][idx]\n",
    "        )  # adjusting the duration with its note value\n",
    "        start_idx = int(torch.round(onset / interval).item())\n",
    "        end_idx = int(torch.round((onset + adjusted_duration) / interval).long().item())\n",
    "        encoding[pitch, start_idx:end_idx] = 1\n",
    "\n",
    "    # Populate the encoding for the annotations\n",
    "    beats, downbeats, time_signatures, key_signatures, onsets_musical, _, hands = annots\n",
    "\n",
    "    for beat in beats:\n",
    "        idx = int(beat / interval)\n",
    "        encoding[128, idx] = 1\n",
    "\n",
    "    for downbeat in downbeats:\n",
    "        idx = int(downbeat / interval)\n",
    "        encoding[129, idx] = 1\n",
    "\n",
    "    for ts in time_signatures:\n",
    "        time, numerator, denominator = ts\n",
    "        idx = int(time / interval)\n",
    "        encoding[130 + numerator - 1, idx] = 1  # Numerator encoding\n",
    "\n",
    "        # Denominator encoding\n",
    "        denominator_indices = {\n",
    "            1: 0,\n",
    "            2: 1,\n",
    "            4: 2,\n",
    "            8: 3,\n",
    "            16: 4,\n",
    "            32: 5,\n",
    "            64: 6,\n",
    "            128: 7,\n",
    "            256: 8,\n",
    "        }\n",
    "        encoding[146 + denominator_indices[denominator], idx] = 1\n",
    "\n",
    "    for ks in key_signatures:\n",
    "        time, key_number = ks\n",
    "        idx = int(time / interval)\n",
    "        encoding[155 + key_number, idx] = 1\n",
    "\n",
    "    for idx, onset in enumerate(annots[4]):\n",
    "        previous_beat = get_previous_beat(midi_data[idx][1], annots[0])\n",
    "        relative_onset = previous_beat + onset\n",
    "        idx = int(relative_onset / interval)\n",
    "        encoding[167, idx] = 1\n",
    "\n",
    "    for hand in hands:\n",
    "        time, hand_type = hand\n",
    "        idx = int(time / interval)\n",
    "        encoding[168, idx] = hand_type\n",
    "\n",
    "    return encoding\n",
    "\n",
    "\n",
    "# Example usage\n",
    "midi_data = [\n",
    "    (60, 0.5, 0.2, 50),  # C4 note with onset at 0.5s, duration 0.2s, and velocity 50\n",
    "    # ... (other notes)\n",
    "]\n",
    "\n",
    "annots = [\n",
    "    [0.3, 0.5, 0.8, 1.5],  # Beats\n",
    "    [0.5],  # Downbeats\n",
    "    [(0, 4, 4), (5, 3, 4)],  # Time signatures\n",
    "    [(0, 5)],  # Key signatures\n",
    "    [0.5],  # Onsets musical\n",
    "    [2],  # Note values (this note will have twice its original length in beats)\n",
    "    [(0.5, 1)],  # Hands\n",
    "]\n",
    "\n",
    "encoded_tensor = midi_to_encoded_with_annots(notes, lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_tensor_structure(tensor_list):\n",
    "    for i, tensor in enumerate(tensor_list):\n",
    "        print(f\"Tensor {i + 1}:\")\n",
    "        print(f\"  Shape: {tensor.shape}\")\n",
    "        print(f\"  Data type: {tensor.dtype}\")\n",
    "        print(f\"  first shape: {tensor[0]}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "inspect_tensor_structure(lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BeatNet.BeatNet import BeatNet\n",
    "\n",
    "estimator = BeatNet(1, mode=\"online\", inference_model=\"PF\", plot=[], thread=False)\n",
    "output = estimator.process(\"example/heartgrace.mp3\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "metronome: AudioSegment = AudioSegment.from_mp3(\"example/metronome.mp3\")\n",
    "music: AudioSegment = AudioSegment.from_mp3(\"example/heartgrace.mp3\")\n",
    "\n",
    "def overlay(music, metronome, positions_ms):\n",
    "    from io import BytesIO\n",
    "    from pydub.utils import audioop\n",
    "\n",
    "    output = BytesIO()\n",
    "    music, metronome = AudioSegment._sync(music, metronome)\n",
    "    sample_width = music.sample_width\n",
    "    spawn = music._spawn\n",
    "    dmetro = metronome._data\n",
    "\n",
    "    last_end_ms = None\n",
    "    for pos_ms in positions_ms:\n",
    "        if last_end_ms is None:\n",
    "            output.write(music[:pos_ms]._data)\n",
    "        else:\n",
    "            output.write(music[last_end_ms:pos_ms]._data)\n",
    "        # drop down to the raw data\n",
    "        dmusic = music[pos_ms:]._data\n",
    "        target_len = min(len(dmetro), len(dmusic))\n",
    "        added = audioop.add(dmusic[:target_len], dmetro[:target_len], sample_width)\n",
    "        output.write(added)\n",
    "        last_end_ms = pos_ms + len(metronome)\n",
    "    return spawn(data=output)\n",
    "\n",
    "output = overlay(music, metronome, output[:, 0] * 1e3)\n",
    "output.export(\"heartgrace_beat_pf.mp3\", format=\"mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visonic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
